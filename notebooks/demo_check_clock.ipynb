{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71edb016",
   "metadata": {},
   "source": [
    "## Demo: Clock check - for offsets in instrument clocks\n",
    "\n",
    "This is an intermediate step between stage1 and stage 2.  We are trying to determine whether the timestamps for any of the instruments on the same mooring are incorrect.  This is slightly faulty because they could *all* be wrong, unless we are comparing against UTC or have more exact timing knowledge.  For more exact timing knowledge, the deployment time and recovery time (anchor release, either dropping from the ship or release from the seabed) have been added to the yaml file in UTC.  This can be compared against the times estimated through lag correlations.\n",
    "\n",
    "### This notebook \n",
    "\n",
    "**It does not change anything in the data files.**  You run this notebook in order to update the field `clock_offset` (in seconds) in the YAML file for each instrument on a mooring.  This is normally due to the instruments being set up incorrectly (i.e., with a clock time that did not match UTC).\n",
    "\n",
    "After determining the appropriate clock offset, then run the stage2 processing to apply the clock offset to the netCDF files for each instrument.\n",
    "\n",
    "Then, running this notebook again using the stage2 files (`*_use.nc`) should predict no additional clock offsets.\n",
    "\n",
    "Clock offset is in integer seconds ADDED to the original instrument time.  I.e., shifts the record later.\n",
    "\n",
    "### Main check\n",
    "\n",
    "- We look at when--according to the instrument clocks--the `temperature` values are cold.  This assumes that in the middle of the record, the temperatures are colder than the near-surface temperatures (may fail for polar deployments).  Cold is within the mean +- 3 * std of the deep values.\n",
    "\n",
    "- Then check when the instrument first reads a temperature within those bounds: `start_time`\n",
    "- And check when the instrument last reads a temperature within those bounds: `end_time`\n",
    "\n",
    "Check whether the first timestamp within the cold water for that instrument is similar in time to the first timestamp for another instrument.  This should be reasonably good at getting large offsets in clocks.\n",
    "\n",
    "### Secondary check\n",
    "\n",
    "- We interpolate data onto a common time grid (rough and ready)\n",
    "- Check for lag correlation between instruments, and use this to estimate an offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff1f4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import yaml\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import importlib\n",
    "from oceanarray import find_deployment\n",
    "\n",
    "from oceanarray import tools\n",
    "\n",
    "moorlist = ['ds2_X_2012','ds2_X_2017','ds2_X_2018',\n",
    "            'ds8_1_2012','ds9_1_2012','ds10_1_2012', 'ds11_1_2012','ds12_1_2012',\n",
    "            'ds13_1_2012','ds14_1_2012','ds15_1_2012','ds16_1_2012','ds17_1_2012',\n",
    "            'ds19_1_2012','ds18_1_2012','ds28_1_2017',\n",
    "            'dsA_1_2018','dsB_1_2018','dsC_1_2018', 'dsD_1_2018','dsE_1_2018','dsF_1_2018',\n",
    "            'dsM1_1_2017','dsM2_1_2017','dsM3_1_2017','dsM4_1_2017','dsM5_1_2017']\n",
    "moorlist = ['dsE_1_2018']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e4a3c8",
   "metadata": {},
   "source": [
    "## Load data for one mooring into datasets, list of xarray datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84055eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the base directory.  raw is a subdirectory from here moor/raw/ and proc is moor/proc\n",
    "basedir = '/Users/eddifying/Dropbox/data/ifmro_mixsed/ds_data_eleanor/'\n",
    "output_path = basedir + 'moor/proc/'\n",
    "\n",
    "# Toggle to load the *_raw.nc or the *_use.nc\n",
    "file_subscript = '_raw'\n",
    "file_subscript = '_use'\n",
    "print(f\"Using files with {file_subscript}\")\n",
    "\n",
    "\n",
    "\n",
    "# Cycle through the yaml and load instrument data into a list of xarray datasets\n",
    "# Enrich the netCDF with information from the yaml file\n",
    "# Find the mooring's processed directory & read the yaml specification\n",
    "name1 = moorlist[0]\n",
    "proc_dir = output_path + name1\n",
    "moor_yaml = proc_dir + '/' + name1 + '.mooring.yaml'\n",
    "with open(moor_yaml, 'r') as f:\n",
    "    moor_yaml_data = yaml.safe_load(f)\n",
    "\n",
    "# For each instrument, load the raw netCDF files and add some metadata from the yaml\n",
    "datasets = []\n",
    "for i in moor_yaml_data['instruments']:\n",
    "    fname = name1 + '_' + str(i['serial']) + file_subscript + '.nc'\n",
    "    rawfile = proc_dir + '/' + i['instrument'] + '/' + fname\n",
    "    if os.path.exists(rawfile):\n",
    "        print(rawfile)\n",
    "        ds1 = xr.open_dataset(rawfile)\n",
    "\n",
    "        if 'InstrDepth' not in ds1.variables and 'depth' in i:\n",
    "            ds1['InstrDepth'] = i['depth']\n",
    "        if 'instrument' not in ds1.variables and 'instrument' in i:\n",
    "            ds1['instrument'] = i['instrument']\n",
    "        if 'serial_number' not in ds1.variables and 'serial' in i:\n",
    "            ds1['serial_number'] = i['serial']\n",
    "        if 'timeS' in ds1.variables:\n",
    "            ds1 = ds1.drop_vars('timeS')\n",
    "        #---------------------------------------------\n",
    "        # Store the data in a list of datasets\n",
    "        datasets.append(ds1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81775c9",
   "metadata": {},
   "source": [
    "# Interpolate dataset\n",
    "\n",
    "Here we interpolate data onto the same time grid to simply checking for clock offsets (in a later step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd66e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect some timing info for each instrument\n",
    "intervals_min = []\n",
    "start_times = []\n",
    "end_times = []\n",
    "# For each dataset, write the coverage\n",
    "for idx, ds in enumerate(datasets):\n",
    "    time = ds['time']\n",
    "    start_time = str(time.values[0])\n",
    "    end_time = str(time.values[-1])\n",
    "    time_interval = (time.values[1] - time.values[0]) / np.timedelta64(1, 'm')\n",
    "    time_interval = np.nanmedian(np.diff(time.values) / np.timedelta64(1, 'm') )\n",
    "    if time_interval > 1:\n",
    "        tstr = f\"{time_interval:1.2f} min\"\n",
    "    else:\n",
    "        tstr = f\"{time_interval * 60:1.2f} sec\"\n",
    "    variables = list(ds.data_vars)\n",
    "    print(f\"Dataset {idx} depth {str(ds['InstrDepth'].values)} [{ds['instrument'].values}:{ds['serial_number'].values}]:\")\n",
    "    print(f\"  Start time: {start_time[0:19]}.  End time:   {end_time[0:19]}.  Time interval: {tstr}\")\n",
    "    print(f\"  Coordinates: {list(ds.coords)}.  Variables: {variables}\")\n",
    "\n",
    "    #---------------------------------------------\n",
    "    # Save the interval for later use\n",
    "    intervals_min.append(time_interval)\n",
    "    start_times.append(time.values[0])\n",
    "    end_times.append(time.values[-1])\n",
    "\n",
    "earliest_start = min(start_times)\n",
    "\n",
    "end_arr = np.array(end_times, dtype='datetime64[ns]')\n",
    "mask = ~np.isnat(end_arr)\n",
    "if mask.any():\n",
    "    med_ns = np.median(end_arr[mask].astype('int64'))  # median in ns\n",
    "    latest_end = np.datetime64(int(med_ns), 'ns')\n",
    "else:\n",
    "    latest_end = np.datetime64('NaT', 'ns')\n",
    "earliest_start = pd.to_datetime(start_times).min().to_datetime64()\n",
    "dt_sec = int(np.nanmedian(intervals_min) * 60)\n",
    "time_grid = np.arange(earliest_start, latest_end, np.timedelta64(dt_sec, 's'))\n",
    "\n",
    "#    latest_end = pd.to_datetime(end_times).median()        # pandas.Timestamp\n",
    "#latest_end = latest_end.to_datetime64()\n",
    "#latest_end = np.median(np.array(end_times))\n",
    "#time_grid = np.arange(earliest_start, latest_end, np.timedelta64(int(np.nanmedian(intervals_min) * 60), 's'))\n",
    "\n",
    "print(f\"Time grid length: {len(time_grid)}\")\n",
    "print(f\"First time: {time_grid[0]}\")\n",
    "print(f\"Last time: {time_grid[-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2442b1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_interp = []\n",
    "for idx, ds in enumerate(datasets):\n",
    "    print(f\"Dataset {idx}:\")\n",
    "    print(f\"  Dimensions: {dict(ds.sizes)},  Coordinates: {list(ds.coords)},  Data variables: {list(ds.data_vars)}\")\n",
    "\n",
    "    # Check if this dataset has a time dimension\n",
    "    if 'time' not in ds.sizes:\n",
    "        print(f\"  WARNING: Dataset {idx} has no time dimension, skipping interpolation\")\n",
    "        continue\n",
    "\n",
    "    # Check if time dimension has more than one element\n",
    "    if ds.sizes['time'] <= 1:\n",
    "        print(f\"  WARNING: Dataset {idx} has only {ds.sizes['time']} time point(s), skipping interpolation\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        # Select only the variables present in the dataset that have time dimension\n",
    "        interp_vars = {}\n",
    "        for var in ds.data_vars:\n",
    "            if 'time' in ds[var].dims:\n",
    "                interp_vars[var] = ds[var].interp(time=time_grid)\n",
    "            else:\n",
    "                interp_vars[var] = ds[var]  # Keep variables without time dimension as is\n",
    "\n",
    "        if interp_vars:  # Only create dataset if we have variables to interpolate\n",
    "            # Create a new dataset with interpolated variables and replace the time coordinate\n",
    "            ds_interp = xr.Dataset(interp_vars, coords={'time': time_grid})\n",
    "\n",
    "            # Add depth and other metadata as coordinates (not variables with time dimension)\n",
    "            if 'InstrDepth' in ds:\n",
    "                ds_interp = ds_interp.assign_coords(depth=ds['InstrDepth'])\n",
    "            if 'clock_offset' in ds:\n",
    "                ds_interp = ds_interp.assign_coords(seconds_offset=ds['clock_offset'])\n",
    "\n",
    "            datasets_interp.append(ds_interp)\n",
    "            #print(f\"  Successfully interpolated dataset {idx}\")\n",
    "        else:\n",
    "            print(f\"  No time-dependent variables found in dataset {idx}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"  ERROR interpolating dataset {idx}: {e}\")\n",
    "        continue\n",
    "\n",
    "print(f\"\\nSuccessfully interpolated {len(datasets_interp)} out of {len(datasets)} datasets\")\n",
    "\n",
    "datasets_interp[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b34a46",
   "metadata": {},
   "source": [
    "# Combine interpolated datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428070e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of variables to keep\n",
    "vars_to_keep = ['temperature', 'salinity', 'conductivity', 'pressure', 'u_velocity','v_velocity']#,'serial_number','InstrDepth']\n",
    "\n",
    "# Remove unwanted variables from each dataset\n",
    "datasets_clean = []\n",
    "for ds in datasets_interp:\n",
    "    ds_sel = ds.drop_vars(['density', 'potential_temperature', 'julian_days_offset','timeS'], errors='ignore')\n",
    "    datasets_clean.append(ds_sel)\n",
    "\n",
    "# Find union of all time coordinates (should be the same for all, but let's check)\n",
    "time_coord = datasets_interp[0]['time']\n",
    "\n",
    "# Prepare data arrays for each variable\n",
    "combined_data = {}\n",
    "N_LEVELS = len(datasets_clean)\n",
    "\n",
    "for var in vars_to_keep:\n",
    "    arrs = []\n",
    "    for ds in datasets_clean:\n",
    "        if var in ds:\n",
    "            arrs.append(ds[var].values)\n",
    "        else:\n",
    "            arrs.append(np.full(time_coord.shape, np.nan))\n",
    "    # Stack along new N_LEVELS dimension\n",
    "    combined_data[var] = (('time', 'N_LEVELS'), np.stack(arrs, axis=-1))\n",
    "\n",
    "# Gather scalar variables for each level\n",
    "depths = []\n",
    "clock_offsets = []\n",
    "serial = []\n",
    "instrtype = []\n",
    "for ds in datasets_clean:\n",
    "    # depth: if missing, use NaN\n",
    "    depths.append(float(ds[\"InstrDepth\"].item()) if \"InstrDepth\" in ds else np.nan)\n",
    "    serial.append(ds['serial_number'].item() if \"serial_number\" in ds else np.nan)\n",
    "    instrtype.append(ds['instrument'].item() if \"instrument\" in ds else 'unknown'  )\n",
    "    # clock offset: prefer 'clock_offset', fall back to legacy 'seconds_offset', else 0\n",
    "    if \"clock_offset\" in ds:\n",
    "        co = ds[\"clock_offset\"].item()\n",
    "    elif \"seconds_offset\" in ds:\n",
    "        co = ds[\"seconds_offset\"].item()\n",
    "    else:\n",
    "        co = 0\n",
    "    # cast safely; NaN -> 0\n",
    "    clock_offsets.append(int(np.rint(co)) if np.isfinite(co) else 0)\n",
    "\n",
    "# Create the combined dataset\n",
    "combined_ds = xr.Dataset(\n",
    "    data_vars=combined_data,\n",
    "    coords={\n",
    "        'time': time_coord,\n",
    "        'N_LEVELS': np.arange(N_LEVELS),\n",
    "        'clock_offset': ('N_LEVELS', np.array(clock_offsets)),\n",
    "        'serial_number': ('N_LEVELS', np.array(serial)),\n",
    "        'nominal_depth': ('N_LEVELS', np.array(depths)),\n",
    "        \"instrument\": (\"N_LEVELS\", np.asarray(instrtype)),  # <-- 1-D\n",
    "\n",
    "\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "combined_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63a8739",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8feba632",
   "metadata": {},
   "source": [
    "## Identify likely start and end of DEPLOYED period (on the bottom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ddf25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(find_deployment)\n",
    "\n",
    "combined_ds = find_deployment.find_deployment(combined_ds,bottom_strategy=\"deployment_bounds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46914e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ac034e",
   "metadata": {},
   "source": [
    "## Plot all the time series of temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd678eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "ds = combined_ds\n",
    "time        = ds[\"time\"].values\n",
    "temp        = ds[\"temperature\"].values\n",
    "split_vals  = ds[\"split_value\"].values\n",
    "instruments = ds[\"instrument\"].values\n",
    "start_times = ds[\"start_time\"].values\n",
    "end_times   = ds[\"end_time\"].values\n",
    "\n",
    "#for i in range(0,1):\n",
    "for i in range(ds.dims[\"N_LEVELS\"]):\n",
    "    fig, ax = plt.subplots(figsize=(10, 4))\n",
    "\n",
    "    ax.plot(time, temp[:, i], label=f\"{instruments[i]}\")\n",
    "    ax.axhline(split_vals[i], color=\"red\", linestyle=\"--\",\n",
    "               label=f\"Split={split_vals[i]:.2f}\")\n",
    "\n",
    "    # vertical lines for start/end times (if not NaT)\n",
    "    if np.isfinite(start_times[i].astype(\"datetime64[ns]\").astype(\"int64\")):\n",
    "        ax.axvline(start_times[i], color=\"green\", linestyle=\"--\", lw=1.5,\n",
    "                   label=\"Start\")\n",
    "        print(f\"start time is {start_times[i]}\")\n",
    "    if np.isfinite(end_times[i].astype(\"datetime64[ns]\").astype(\"int64\")):\n",
    "        ax.axvline(end_times[i], color=\"blue\", linestyle=\"--\", lw=1.5,\n",
    "                   label=\"End\")\n",
    "\n",
    "\n",
    "    ax.set_title(f\"Instrument {i}: {instruments[i]}\")\n",
    "    ax.set_xlabel(\"Time\")\n",
    "    ax.set_ylabel(\"Temperature\")\n",
    "    ax.legend()\n",
    "    ax.grid(True, linestyle=\":\")\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643937d2",
   "metadata": {},
   "source": [
    "## Create a table of start/end and offset\n",
    "\n",
    "Use the **negative** of the offset as the clock_offset in the yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51a5e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "ds = combined_ds  # or your dataset\n",
    "\n",
    "# --- pull times ---\n",
    "start_times = pd.to_datetime(ds[\"start_time\"].values)\n",
    "end_times   = pd.to_datetime(ds[\"end_time\"].values)\n",
    "\n",
    "# finite masks\n",
    "f_start = np.isfinite(start_times)\n",
    "f_end   = np.isfinite(end_times)\n",
    "\n",
    "# --- initial refs (global) just to compute provisional offsets for clustering ---\n",
    "ref_start0 = start_times[f_start].min()\n",
    "ref_end0   = end_times[f_end].max()\n",
    "\n",
    "start_off0 = np.full(start_times.shape, np.nan, float)\n",
    "end_off0   = np.full(end_times.shape,   np.nan, float)\n",
    "start_off0[f_start] = (start_times[f_start] - ref_start0) / np.timedelta64(1, \"s\")\n",
    "end_off0[f_end]     = (end_times[f_end]   - ref_end0)     / np.timedelta64(1, \"s\")\n",
    "\n",
    "# --- consensus group on start offsets (bin to nearest 60 s) ---\n",
    "bin_width = 60.0  # seconds\n",
    "vals = start_off0[np.isfinite(start_off0)]\n",
    "if vals.size == 0:\n",
    "    raise RuntimeError(\"No finite start offsets to form consensus.\")\n",
    "\n",
    "vmin, vmax = vals.min(), vals.max()\n",
    "bins = np.arange(vmin - bin_width, vmax + 2*bin_width, bin_width)\n",
    "hist, edges = np.histogram(vals, bins=bins)\n",
    "k = np.argmax(hist)\n",
    "lo, hi = edges[k], edges[k+1]\n",
    "in_consensus = (start_off0 >= lo) & (start_off0 < hi)\n",
    "\n",
    "idx_consensus = np.where(in_consensus & f_start & f_end)[0]  # require both times finite\n",
    "if idx_consensus.size == 0:\n",
    "    # fallback: use all in the winning bin even if some lack end_times\n",
    "    idx_consensus = np.where(in_consensus & f_start)[0]\n",
    "# --- redefine refs from consensus group only ---\n",
    "ref_start = start_times[idx_consensus].min()\n",
    "ref_end   = end_times[idx_consensus].max()\n",
    "\n",
    "# --- recompute offsets relative to NEW refs ---\n",
    "start_off = (start_times - ref_start) / np.timedelta64(1, \"s\")\n",
    "end_off   = (end_times   - ref_end)   / np.timedelta64(1, \"s\")\n",
    "avg_off   = (start_off + end_off) / 2.0\n",
    "diff_off  = start_off - end_off       # start vs end disagreement (drift hint)\n",
    "\n",
    "# --- optional: drift rate (s/day), skip ones without both times ---\n",
    "dur = (end_times - start_times) / np.timedelta64(1, \"s\")\n",
    "drift_rate_per_day = np.full_like(avg_off, np.nan, dtype=float)\n",
    "ok = np.isfinite(start_off) & np.isfinite(end_off) & np.isfinite(dur) & (dur > 0)\n",
    "drift_rate_per_day[ok] = (end_off[ok] - start_off[ok]) / dur[ok] * 86400.0\n",
    "\n",
    "# --- print summary ---\n",
    "N = ds.sizes[\"N_LEVELS\"]\n",
    "labels = ds[\"instrument\"].values if \"instrument\" in ds else np.arange(N)\n",
    "serial = ds[\"serial_number\"].values if \"serial_number\" in ds else \"0\"\n",
    "print(f\"Consensus group size: {idx_consensus.size}\")\n",
    "print(f\"Consensus-derived refs -> ref_start={ref_start}, ref_end={ref_end}\\n\")\n",
    "\n",
    "for i in range(N):\n",
    "    tag = \"REF\" if i in idx_consensus else \"-\"\n",
    "    s = start_off[i]; e = end_off[i]; a = avg_off[i]; d = diff_off[i]; dr = drift_rate_per_day[i]\n",
    "    print(\n",
    "        f\"{i:02d}: {str(labels[i]):8s}/{str(serial[i]):6s} | \"\n",
    "        f\"start={pd.to_datetime(start_times[i])} ({s:+8.0f}s) | \"\n",
    "        f\"end={pd.to_datetime(end_times[i])} ({e:+8.0f}s) | \"\n",
    "        f\"avg={a:+8.0f}s | diff={d:+6.0f}s | drift={'nan' if not np.isfinite(dr) else f'{dr:+.2f} s/day'} | {tag}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8317d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdffd3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trim combined_ds to the period between start_time and end_time\n",
    "#combined_ds\n",
    "#combined_ds = combined_ds.where(combined_ds['time']>combined_ds.attrs['start_time'], drop=True)\n",
    "#combined_ds = combined_ds.where(combined_ds['time']<combined_ds.attrs['end_time'], drop=True)\n",
    "\n",
    "\n",
    "# Plot a basic variable\n",
    "time_interval = np.nanmedian(np.diff(combined_ds['time'].values) / np.timedelta64(1, 'm'))\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "for i in range(combined_ds.sizes['N_LEVELS']):\n",
    "    plt.plot(combined_ds['time'], combined_ds['temperature'][:, i], label=f'Level {i} ({depths[i]} m)')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Temperature (°C)')\n",
    "plt.title('Temperature Time Series for All Levels')\n",
    "plt.legend(loc='upper right', ncol=2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.ylim(0,5)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b012ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.reload(tools)\n",
    "time_interval = np.nanmedian(np.diff(combined_ds['time'].values) / np.timedelta64(1, 's'))\n",
    "\n",
    "# Subsample time series\n",
    "sub_sample = 5\n",
    "\n",
    "\n",
    "ref_index = 5\n",
    "\n",
    "n_full = len(combined_ds['temperature'][:, ref_index].values)\n",
    "ref_temp_sub = combined_ds['temperature'][:, ref_index].values[::sub_sample]\n",
    "n_sub = len(ref_temp_sub)\n",
    "print(f\"Full length is {n_full}.  Subsampled length is {n_sub}.\")\n",
    "\n",
    "max_lag_sub = n_sub // 5\n",
    "max_lag_sub = n_sub // 5\n",
    "lags_sub = np.arange(-max_lag_sub, max_lag_sub + 1)\n",
    "lags = []\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "for i in range(0, N_LEVELS):\n",
    "    serial = combined_ds['serial_number'][i].values\n",
    "    temp_i_sub = combined_ds['temperature'][:, i].values[::sub_sample]\n",
    "    coff = combined_ds['clock_offset'][i].values\n",
    "    corrs_sub = tools.lag_correlation(ref_temp_sub, temp_i_sub, max_lag_sub)\n",
    "    # Find maximum correlation and corresponding lag\n",
    "    max_corr_idx = np.nanargmax(corrs_sub)\n",
    "    max_corr = corrs_sub[max_corr_idx]\n",
    "    max_lag = lags_sub[max_corr_idx]\n",
    "    dt_sub = sub_sample * time_interval\n",
    "    print(f\"Level {i+1} (#{serial}): max corr = {max_corr:.3f} @lag {max_lag} --> clock_offset (sec): {max_lag*dt_sub} + {coff} = {max_lag*dt_sub+coff}s\")\n",
    "    plt.plot(lags_sub*dt_sub, corrs_sub, label=f'Level {i+1} ({depths[i]} m)')\n",
    "\n",
    "    lags.append(max_lag)\n",
    "\n",
    "combined_ds['lags'] = ('N_LEVELS', lags)\n",
    "print(f\"Enter the summed value, no sign change, in the yaml as clock_offset: XXXX\")\n",
    "\n",
    "plt.xlabel('Lag (s)')\n",
    "plt.ylabel('Correlation')\n",
    "plt.title('Coarse Lag Correlation: Temperature N_LEVEL[0] vs Others (Subsampled)')\n",
    "plt.legend(loc='upper right', ncol=2)\n",
    "plt.grid()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05db182c",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4c6966",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f233aed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4cddaa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.11.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
