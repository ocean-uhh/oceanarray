{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Step 1: Time Gridding and Optional Filtering Demo\n",
        "\n",
        "This notebook demonstrates the Step 1 processing workflow for mooring data:\n",
        "- Loading multiple instrument datasets\n",
        "- Optional time-domain filtering (applied BEFORE interpolation)\n",
        "- Interpolating onto a common time grid\n",
        "- Combining into a unified mooring dataset\n",
        "\n",
        "**Key Point**: Filtering is applied to individual instrument records on their native time grids BEFORE interpolation to preserve data integrity.\n",
        "\n",
        "Version: 1.0  \n",
        "Date: 2025-09-07"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import xarray as xr\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pathlib import Path\n",
        "import yaml\n",
        "\n",
        "# Import the time gridding module\n",
        "from oceanarray.time_gridding import (\n",
        "    TimeGriddingProcessor,\n",
        "    time_gridding_mooring,\n",
        "    process_multiple_moorings_time_gridding\n",
        ")\n",
        "\n",
        "# Set up plotting\n",
        "plt.style.use('default')\n",
        "sns.set_palette(\"husl\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Data Setup and Configuration\n",
        "\n",
        "First, let's set up our data paths and examine the mooring configuration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set your data paths here\n",
        "basedir = '/Users/eddifying/Dropbox/data/ifmro_mixsed/ds_data_eleanor/'\n",
        "mooring_name = 'dsE_1_2018'\n",
        "\n",
        "# Construct paths\n",
        "proc_dir = Path(basedir) / 'moor' / 'proc' / mooring_name\n",
        "config_file = proc_dir / f\"{mooring_name}.mooring.yaml\"\n",
        "\n",
        "print(f\"Processing directory: {proc_dir}\")\n",
        "print(f\"Configuration file: {config_file}\")\n",
        "print(f\"Config exists: {config_file.exists()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load and examine the mooring configuration\n",
        "if config_file.exists():\n",
        "    with open(config_file, 'r') as f:\n",
        "        config = yaml.safe_load(f)\n",
        "\n",
        "    print(\"Mooring Configuration:\")\n",
        "    print(f\"Name: {config['name']}\")\n",
        "    print(f\"Water depth: {config.get('waterdepth', 'unknown')} m\")\n",
        "    print(f\"Location: {config.get('latitude', 'unknown')}°N, {config.get('longitude', 'unknown')}°E\")\n",
        "    print(f\"\\nInstruments ({len(config.get('instruments', []))}):\")\n",
        "\n",
        "    for i, inst in enumerate(config.get('instruments', [])):\n",
        "        print(f\"  {i+1}. {inst.get('instrument', 'unknown')} \"\n",
        "              f\"(serial: {inst.get('serial num.', 'unknown')}) at {inst.get('depth', 'unknown')} m\")\n",
        "else:\n",
        "    print(\"Configuration file not found!\")\n",
        "    print(\"Please check your data path and mooring name.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Examine Individual Instrument Files\n",
        "\n",
        "Let's look at the individual instrument files before processing to understand the different sampling rates and data characteristics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Find and examine individual instrument files\n",
        "file_suffix = \"_use\"\n",
        "instrument_files = []\n",
        "instrument_datasets = []\n",
        "rows = []\n",
        "\n",
        "if config_file.exists():\n",
        "    for inst_config in config.get(\"instruments\", []):\n",
        "        instrument_type = inst_config.get(\"instrument\", \"unknown\")\n",
        "        serial = inst_config.get(\"serial\", 0)\n",
        "        depth = inst_config.get(\"depth\", 0)\n",
        "\n",
        "        # Look for the file\n",
        "        filename = f\"{mooring_name}_{serial}{file_suffix}.nc\"\n",
        "        filepath = proc_dir / instrument_type / filename\n",
        "\n",
        "        if filepath.exists():\n",
        "            ds = xr.open_dataset(filepath)\n",
        "            instrument_files.append(filepath)\n",
        "            instrument_datasets.append(ds)\n",
        "\n",
        "            # Time coverage\n",
        "            t0, t1 = ds.time.values[0], ds.time.values[-1]\n",
        "            npoints = len(ds.time)\n",
        "\n",
        "            # Median sampling interval\n",
        "            time_diff = np.diff(ds.time.values) / np.timedelta64(1, \"m\")  # in minutes\n",
        "            median_interval = np.nanmedian(time_diff)\n",
        "            if median_interval > 1:\n",
        "                sampling = f\"{median_interval:.1f} min\"\n",
        "            else:\n",
        "                sampling = f\"{median_interval*60:.1f} sec\"\n",
        "\n",
        "            # Collect a row for the table\n",
        "            rows.append(\n",
        "                {\n",
        "                    \"Instrument\": instrument_type,\n",
        "                    \"Serial\": serial,\n",
        "                    \"Depth [m]\": depth,\n",
        "                    \"File\": filepath.name,\n",
        "                    \"Start\": str(t0)[:19],\n",
        "                    \"End\": str(t1)[:19],\n",
        "                    \"Points\": npoints,\n",
        "                    \"Sampling\": sampling,\n",
        "                    \"Variables\": \", \".join(list(ds.data_vars)),\n",
        "                }\n",
        "            )\n",
        "        else:\n",
        "            rows.append(\n",
        "                {\n",
        "                    \"Instrument\": instrument_type,\n",
        "                    \"Serial\": serial,\n",
        "                    \"Depth [m]\": depth,\n",
        "                    \"File\": \"MISSING\",\n",
        "                    \"Start\": \"\",\n",
        "                    \"End\": \"\",\n",
        "                    \"Points\": 0,\n",
        "                    \"Sampling\": \"\",\n",
        "                    \"Variables\": \"\",\n",
        "                }\n",
        "            )\n",
        "\n",
        "    # Make a DataFrame summary\n",
        "    summary = pd.DataFrame(rows)\n",
        "    pd.set_option(\"display.max_colwidth\", 80)  # allow long var lists\n",
        "    print(summary.to_markdown(index=False))\n",
        "\n",
        "    print(f\"\\nFound {len(instrument_datasets)} instrument datasets\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Process with Time Gridding (No Filtering)\n",
        "\n",
        "First, let's process the mooring without any filtering to see the basic time gridding functionality."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Process without filtering\n",
        "print(\"Processing mooring with time gridding only (no filtering)...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "result = time_gridding_mooring(mooring_name, basedir, file_suffix='_use')\n",
        "\n",
        "print(f\"\\nProcessing result: {'SUCCESS' if result else 'FAILED'}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load and examine the combined dataset\n",
        "output_file = proc_dir / f\"{mooring_name}_mooring_use.nc\"\n",
        "\n",
        "if output_file.exists():\n",
        "    print(f\"Output file exists: {output_file}\")\n",
        "\n",
        "    # Load the combined dataset\n",
        "    combined_ds = xr.open_dataset(output_file)\n",
        "else:\n",
        "    print(\"Output file not found - processing may have failed\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Visualize Combined Dataset\n",
        "\n",
        "Let's plot the combined dataset to see how the different instruments look on the common time grid."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_combined_timeseries(\n",
        "    combined_ds,\n",
        "    variables=(\"temperature\", \"salinity\", \"pressure\"),\n",
        "    cmap_name=\"viridis\",\n",
        "    line_alpha=0.8,\n",
        "    line_width=1.2,\n",
        "    percentile_limits=(1, 99),\n",
        "):\n",
        "    \"\"\"\n",
        "    Plot selected variables from a combined mooring dataset as stacked time series.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    combined_ds : xarray.Dataset\n",
        "        Must have dims: time, N_LEVELS. Optional coords: nominal_depth, serial_number.\n",
        "    variables : iterable[str]\n",
        "        Variable names to try to plot (if present in dataset).\n",
        "    cmap_name : str\n",
        "        Matplotlib colormap name for coloring by instrument level.\n",
        "    line_alpha : float\n",
        "        Line transparency.\n",
        "    line_width : float\n",
        "        Line width.\n",
        "    percentile_limits : (low, high)\n",
        "        Percentiles to use for automatic y-limits (e.g., (1, 99)).\n",
        "    \"\"\"\n",
        "    if combined_ds is None:\n",
        "        print(\"Combined dataset not available.\")\n",
        "        return None, None\n",
        "    n_levels = combined_ds.sizes.get(\"N_LEVELS\")\n",
        "    if n_levels is None:\n",
        "        raise ValueError(\"Dataset must contain dimension 'N_LEVELS'.\")\n",
        "\n",
        "    available = [v for v in variables if v in combined_ds.data_vars]\n",
        "    if not available:\n",
        "        print(\"No requested variables found to plot.\")\n",
        "        return None, None\n",
        "\n",
        "    # Colors by level\n",
        "    cmap = plt.get_cmap(cmap_name)\n",
        "    colors = cmap(np.linspace(0, 1, n_levels))\n",
        "\n",
        "    fig, axes = plt.subplots(\n",
        "        len(available), 1, figsize=(14, 3.6 * len(available)), sharex=True, constrained_layout=True\n",
        "    )\n",
        "    if len(available) == 1:\n",
        "        axes = [axes]\n",
        "\n",
        "    depth_arr = combined_ds.get(\"nominal_depth\")\n",
        "    serial_arr = combined_ds.get(\"serial_number\")\n",
        "\n",
        "    first_axis = True\n",
        "    for ax, var in zip(axes, available):\n",
        "        values_for_limits = []\n",
        "        for level in range(n_levels):\n",
        "            depth = None if depth_arr is None else depth_arr.values[level]\n",
        "            serial = None if serial_arr is None else serial_arr.values[level]\n",
        "            label = None\n",
        "            if first_axis:\n",
        "                if depth is not None and np.isfinite(depth):\n",
        "                    label = f\"Serial {serial} ({int(depth)} m)\" if serial is not None else f\"({int(depth)} m)\"\n",
        "                elif serial is not None:\n",
        "                    label = f\"Serial {serial}\"\n",
        "\n",
        "            da = combined_ds[var].isel(N_LEVELS=level)\n",
        "            da = da.where(np.isfinite(da), drop=True)\n",
        "            if da.size == 0:\n",
        "                continue\n",
        "\n",
        "            values_for_limits.append(da.values)\n",
        "\n",
        "            ax.plot(\n",
        "                da[\"time\"].values,\n",
        "                da.values,\n",
        "                color=colors[level],\n",
        "                alpha=line_alpha,\n",
        "                linewidth=line_width,\n",
        "                label=label,\n",
        "            )\n",
        "\n",
        "        # Set labels and grid\n",
        "        ax.set_ylabel(var.replace(\"_\", \" \").title())\n",
        "        ax.grid(True, alpha=0.3)\n",
        "        ax.set_title(f\"{var.replace('_', ' ').title()} — Combined Time Grid\")\n",
        "\n",
        "        # Legend only once\n",
        "        if first_axis:\n",
        "            ax.legend(ncol=3, fontsize=8, loc=\"upper right\", frameon=False)\n",
        "            first_axis = False\n",
        "\n",
        "        # Auto y-limits based on percentiles\n",
        "        if values_for_limits:\n",
        "            flat = np.concatenate(values_for_limits)\n",
        "            low, high = np.nanpercentile(flat, percentile_limits)\n",
        "            ax.set_ylim(low, high)\n",
        "\n",
        "    axes[-1].set_xlabel(\"Time\")\n",
        "    return fig, axes\n",
        "\n",
        "# Usage:\n",
        "if 'combined_ds' in locals():\n",
        "    plot_combined_timeseries(combined_ds)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Process with Low-pass Filtering (RAPID-style)\n",
        "\n",
        "Now let's apply the RAPID-style 2-day low-pass filter to remove tidal and inertial variability. Remember: **filtering is applied to individual instruments BEFORE interpolation**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Process with RAPID-style low-pass filtering\n",
        "print(\"Processing mooring with 2-day low-pass filtering (RAPID-style)...\")\n",
        "print(\"=\"*60)\n",
        "print(\"IMPORTANT: Filtering is applied to each instrument on its native time grid\")\n",
        "print(\"BEFORE interpolation to preserve data integrity.\")\n",
        "print()\n",
        "\n",
        "filter_params = {\n",
        "    'cutoff_days': 2.0,  # 2-day cutoff\n",
        "    'order': 6           # 6th order Butterworth\n",
        "}\n",
        "\n",
        "result_filtered = time_gridding_mooring(\n",
        "    mooring_name, basedir,\n",
        "    file_suffix='_use',\n",
        "    filter_type='lowpass',\n",
        "    filter_params=filter_params\n",
        ")\n",
        "\n",
        "print(f\"\\nFiltered processing result: {'SUCCESS' if result_filtered else 'FAILED'}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the filtered dataset\n",
        "filtered_output_file = proc_dir / f\"{mooring_name}_mooring_use_lowpass.nc\"\n",
        "\n",
        "if filtered_output_file.exists():\n",
        "    print(f\"Filtered output file created: {filtered_output_file}\")\n",
        "\n",
        "    # Load the filtered dataset\n",
        "    filtered_ds = xr.open_dataset(filtered_output_file)\n",
        "\n",
        "    print(\"\\nFiltered Dataset Attributes:\")\n",
        "    filter_attrs = {k: v for k, v in filtered_ds.attrs.items()\n",
        "                   if 'filter' in k.lower()}\n",
        "    for key, value in filter_attrs.items():\n",
        "        print(f\"  {key}: {value}\")\n",
        "\n",
        "    print(f\"\\nDataset shape: {dict(filtered_ds.dims)}\")\n",
        "else:\n",
        "    print(\"Filtered output file not found\")\n",
        "    filtered_ds = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Compare Filtered vs Unfiltered Data\n",
        "\n",
        "Let's compare the original and filtered data to see the effect of the low-pass filter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if 'combined_ds' in locals() and filtered_ds is not None:\n",
        "    # Compare filtered vs unfiltered for a subset of data\n",
        "    # Select a 10-day window for detailed comparison\n",
        "    start_time = combined_ds.time.values[len(combined_ds.time)//4]  # Start 1/4 through\n",
        "    end_time = start_time + np.timedelta64(10, 'D')  # 10-day window\n",
        "\n",
        "    # Select subset\n",
        "    subset_orig = combined_ds.sel(time=slice(start_time, end_time))\n",
        "    subset_filt = filtered_ds.sel(time=slice(start_time, end_time))\n",
        "\n",
        "    # Plot comparison for temperature\n",
        "    if 'temperature' in subset_orig.data_vars:\n",
        "        fig, axes = plt.subplots(2, 1, figsize=(14, 8), sharex=True)\n",
        "\n",
        "        # Choose a representative level (first one with data)\n",
        "        level = 0\n",
        "        depth = subset_orig.nominal_depth.values[level]\n",
        "        serial = subset_orig.serial_number.values[level]\n",
        "\n",
        "        # Original data\n",
        "        orig_temp = subset_orig.temperature.isel(N_LEVELS=level)\n",
        "        axes[0].plot(orig_temp.time, orig_temp, 'b-', alpha=0.7, linewidth=1,\n",
        "                    label='Original')\n",
        "        axes[0].set_ylabel('Temperature (°C)')\n",
        "        axes[0].set_title(f'Original Data - Serial {serial} at {depth}m')\n",
        "        axes[0].grid(True, alpha=0.3)\n",
        "        axes[0].legend()\n",
        "\n",
        "        # Filtered data\n",
        "        filt_temp = subset_filt.temperature.isel(N_LEVELS=level)\n",
        "        axes[1].plot(filt_temp.time, filt_temp, 'r-', alpha=0.7, linewidth=1.5,\n",
        "                    label='2-day Low-pass Filtered')\n",
        "        axes[1].set_ylabel('Temperature (°C)')\n",
        "        axes[1].set_xlabel('Time')\n",
        "        axes[1].set_title(f'Filtered Data - Serial {serial} at {depth}m')\n",
        "        axes[1].grid(True, alpha=0.3)\n",
        "        axes[1].legend()\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        # Overlay comparison\n",
        "        fig, ax = plt.subplots(1, 1, figsize=(14, 6))\n",
        "        ax.plot(orig_temp.time, orig_temp, 'b-', alpha=0.5, linewidth=0.8,\n",
        "                label='Original')\n",
        "        ax.plot(filt_temp.time, filt_temp, 'r-', alpha=0.8, linewidth=2,\n",
        "                label='2-day Low-pass Filtered')\n",
        "        ax.set_ylabel('Temperature (°C)')\n",
        "        ax.set_xlabel('Time')\n",
        "        ax.set_title(f'Filtering Comparison - Serial {serial} at {depth}m')\n",
        "        ax.legend()\n",
        "        ax.grid(True, alpha=0.3)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "else:\n",
        "    print(\"Cannot compare - one or both datasets not available\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Spectral Analysis: Effect of Filtering\n",
        "\n",
        "Let's examine the spectral characteristics to see how the filter affects different frequency components."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if 'combined_ds' in locals() and filtered_ds is not None:\n",
        "    from scipy import signal\n",
        "\n",
        "    # Select a level with good data coverage\n",
        "    level = 0\n",
        "\n",
        "    # Get temperature data\n",
        "    if 'temperature' in combined_ds.data_vars:\n",
        "        orig_temp = combined_ds.temperature.isel(N_LEVELS=level).dropna('time')\n",
        "        filt_temp = filtered_ds.temperature.isel(N_LEVELS=level).dropna('time')\n",
        "\n",
        "        if len(orig_temp) > 100:  # Ensure sufficient data\n",
        "            # Calculate sampling rate\n",
        "            dt_hours = float(np.median(np.diff(orig_temp.time.values)) / np.timedelta64(1, 'h'))\n",
        "            fs = 1.0 / dt_hours  # samples per hour\n",
        "\n",
        "            # Compute power spectral density\n",
        "            f_orig, psd_orig = signal.welch(orig_temp.values, fs=fs, nperseg=min(256, len(orig_temp)//4))\n",
        "            f_filt, psd_filt = signal.welch(filt_temp.values, fs=fs, nperseg=min(256, len(filt_temp)//4))\n",
        "\n",
        "            # Convert frequency to period in days\n",
        "            period_orig = 1.0 / (f_orig * 24)  # days\n",
        "            period_filt = 1.0 / (f_filt * 24)  # days\n",
        "\n",
        "            # Plot power spectral density\n",
        "            fig, ax = plt.subplots(1, 1, figsize=(12, 6))\n",
        "\n",
        "            ax.loglog(period_orig[1:], psd_orig[1:], 'b-', alpha=0.7,\n",
        "                     label='Original', linewidth=1.5)\n",
        "            ax.loglog(period_filt[1:], psd_filt[1:], 'r-', alpha=0.8,\n",
        "                     label='2-day Low-pass Filtered', linewidth=2)\n",
        "\n",
        "            # Mark important periods\n",
        "            ax.axvline(2.0, color='gray', linestyle='--', alpha=0.7,\n",
        "                      label='2-day cutoff')\n",
        "            ax.axvline(1.0, color='gray', linestyle=':', alpha=0.7,\n",
        "                      label='1-day (diurnal)')\n",
        "            ax.axvline(0.5, color='gray', linestyle=':', alpha=0.7,\n",
        "                      label='12-hour (semidiurnal)')\n",
        "\n",
        "            ax.set_xlabel('Period (days)')\n",
        "            ax.set_ylabel('Power Spectral Density')\n",
        "            ax.set_title('Spectral Analysis: Effect of 2-day Low-pass Filter')\n",
        "            ax.legend()\n",
        "            ax.grid(True, alpha=0.3)\n",
        "            ax.set_xlim(0.1, 100)\n",
        "\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "\n",
        "            print(f\"Spectral analysis completed for level {level}\")\n",
        "            print(f\"Sampling rate: {dt_hours:.2f} hours\")\n",
        "            print(f\"Data length: {len(orig_temp)} points\")\n",
        "        else:\n",
        "            print(\"Insufficient data for spectral analysis\")\n",
        "    else:\n",
        "        print(\"Temperature data not available for spectral analysis\")\n",
        "else:\n",
        "    print(\"Cannot perform spectral analysis - datasets not available\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Multiple Mooring Processing Example\n",
        "\n",
        "The time gridding module also supports batch processing of multiple moorings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Base directory containing the mooring data\n",
        "basedir = '/Users/eddifying/Dropbox/data/ifmro_mixsed/ds_data_eleanor/'\n",
        "\n",
        "# Example of processing multiple moorings\n",
        "# (This will only work if you have multiple moorings in your dataset)\n",
        "\n",
        "# List available moorings\n",
        "moor_base = Path(basedir) / 'moor' / 'proc'\n",
        "available_moorings = [d.name for d in moor_base.iterdir() if d.is_dir()]\n",
        "\n",
        "print(f\"Available moorings in {moor_base}:\")\n",
        "for mooring in available_moorings[:5]:  # Show first 5\n",
        "    print(f\"  - {mooring}\")\n",
        "\n",
        "if len(available_moorings) > 5:\n",
        "    print(f\"  ... and {len(available_moorings)-5} more\")\n",
        "\n",
        "# Example batch processing (commented out to avoid running on all moorings)\n",
        "moorings_to_process = ['dsE_1_2018']  # Add your mooring names\n",
        "\n",
        "results = process_multiple_moorings_time_gridding(\n",
        "    moorings_to_process,\n",
        "    basedir,\n",
        ")\n",
        "\n",
        "print(\"Batch processing results:\")\n",
        "for mooring, success in results.items():\n",
        "    status = \"SUCCESS\" if success else \"FAILED\"\n",
        "    print(f\"  {mooring}: {status}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv (3.11.7)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
