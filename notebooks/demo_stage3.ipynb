{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71edb016",
   "metadata": {},
   "source": [
    "## Demo: Stage3 - Combine individual instruments to one xarray dataset\n",
    "\n",
    "Here we want to treat one mooring as one dataset.\n",
    "\n",
    "The time sampling differs between instruments -- for the *first guess* (24 Aug 2025) we will use a simple interpolation onto a common grid.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff1f4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import yaml\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "from datetime import datetime\n",
    "from ctd_tools.writers import NetCdfWriter\n",
    "\n",
    "from oceanarray import tools\n",
    "\n",
    "moorlist = ['ds2_X_2012','ds2_X_2017','ds2_X_2018',\n",
    "            'ds8_1_2012','ds9_1_2012','ds10_1_2012', 'ds11_1_2012','ds12_1_2012',\n",
    "            'ds13_1_2012','ds14_1_2012','ds15_1_2012','ds16_1_2012','ds17_1_2012',\n",
    "            'ds19_1_2012','ds18_1_2012','ds28_1_2017',\n",
    "            'dsA_1_2018','dsB_1_2018','dsC_1_2018', 'dsD_1_2018','dsE_1_2018','dsF_1_2018',\n",
    "            'dsM1_1_2017','dsM2_1_2017','dsM3_1_2017','dsM4_1_2017','dsM5_1_2017']\n",
    "moorlist = ['dsE_1_2018']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e4a3c8",
   "metadata": {},
   "source": [
    "## Load data for one mooring into datasets, list of xarray datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84055eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the base directory.  raw is a subdirectory from here moor/raw/ and proc is moor/proc\n",
    "basedir = '/Users/eddifying/Dropbox/data/ifmro_mixsed/ds_data_eleanor/'\n",
    "output_path = basedir + 'moor/proc/'\n",
    "\n",
    "# Toggle to load the *_raw.nc or the *_use.nc\n",
    "file_subscript = '_use'\n",
    "print(f\"Using files with {file_subscript}\")\n",
    "\n",
    "\n",
    "\n",
    "# Cycle through the yaml and load instrument data into a list of xarray datasets\n",
    "# Enrich the netCDF with information from the yaml file\n",
    "# Find the mooring's processed directory & read the yaml specification\n",
    "name1 = moorlist[0]\n",
    "proc_dir = output_path + name1\n",
    "moor_yaml = proc_dir + '/' + name1 + '.mooring.yaml'\n",
    "with open(moor_yaml, 'r') as f:\n",
    "    moor_yaml_data = yaml.safe_load(f)\n",
    "\n",
    "# For each instrument, load the raw netCDF files and add some metadata from the yaml\n",
    "datasets = []\n",
    "for i in moor_yaml_data['instruments']:\n",
    "    fname = name1 + '_' + str(i['serial']) + file_subscript + '.nc'\n",
    "    usefile = proc_dir + '/' + i['instrument'] + '/' + fname\n",
    "\n",
    "    if os.path.exists(usefile):\n",
    "        print(usefile)\n",
    "        ds1 = xr.open_dataset(usefile)\n",
    "\n",
    "        if 'InstrDepth' not in ds1.variables and 'depth' in i:\n",
    "            ds1['InstrDepth'] = i['depth']\n",
    "        if 'instrument' not in ds1.variables and 'instrument' in i:\n",
    "            ds1['instrument'] = i['instrument']\n",
    "        if 'serial_number' not in ds1.variables and 'serial' in i:\n",
    "            ds1['serial_number'] = i['serial']\n",
    "        if 'timeS' in ds1.variables:\n",
    "            ds1 = ds1.drop_vars('timeS')\n",
    "        #---------------------------------------------\n",
    "        # Store the data in a list of datasets\n",
    "        datasets.append(ds1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915060e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect some timing info for each instrument\n",
    "intervals_min = []\n",
    "start_times = []\n",
    "end_times = []\n",
    "# For each dataset, write the coverage\n",
    "for idx, ds in enumerate(datasets):\n",
    "    time = ds['time']\n",
    "    start_time = str(time.values[0])\n",
    "    end_time = str(time.values[-1])\n",
    "    time_interval = (time.values[1] - time.values[0]) / np.timedelta64(1, 'm')\n",
    "    time_interval = np.nanmedian(np.diff(time.values) / np.timedelta64(1, 'm') )\n",
    "    if time_interval > 1:\n",
    "        tstr = f\"{time_interval:1.2f} min\"\n",
    "    else:\n",
    "        tstr = f\"{time_interval * 60:1.2f} sec\"\n",
    "    variables = list(ds.data_vars)\n",
    "    print(f\"Dataset {idx} depth {str(ds['InstrDepth'].values)} [{ds['instrument'].values}:{ds['serial_number'].values}]:\")\n",
    "    print(f\"  Start time: {start_time[0:19]}.  End time:   {end_time[0:19]}.  Time interval: {tstr}\")\n",
    "    print(f\"  Coordinates: {list(ds.coords)}.  Variables: {variables}\")\n",
    "\n",
    "    #---------------------------------------------\n",
    "    # Save the interval for later use\n",
    "    intervals_min.append(time_interval)\n",
    "    start_times.append(time.values[0])\n",
    "    end_times.append(time.values[-1])\n",
    "\n",
    "earliest_start = min(start_times)\n",
    "latest_end = max(end_times)\n",
    "time_grid = np.arange(earliest_start, latest_end, np.timedelta64(int(np.nanmedian(intervals_min) * 60), 's'))\n",
    "\n",
    "print(f\"Time grid length: {len(time_grid)}\")\n",
    "print(f\"First time: {time_grid[0]}\")\n",
    "print(f\"Last time: {time_grid[-1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81775c9",
   "metadata": {},
   "source": [
    "# Interpolate dataset\n",
    "\n",
    "Here we interpolate data onto the same time grid to simply checking for clock offsets (in a later step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd66e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with xr.set_options(keep_attrs=True):\n",
    "    datasets_interp = []\n",
    "    for idx, ds in enumerate(datasets):\n",
    "        if 'time' not in ds.sizes or ds.sizes['time'] <= 1:\n",
    "            continue\n",
    "\n",
    "        # interpolate the whole dataset at once\n",
    "        ds_i = ds.interp(time=time_grid)\n",
    "\n",
    "        # preserve global attrs (Dataset.interp can drop them)\n",
    "        ds_i.attrs = dict(ds.attrs)\n",
    "\n",
    "        # keep coord attrs too (optional)\n",
    "        if 'time' in ds.coords and ds.time.attrs:\n",
    "            ds_i.time.attrs = dict(ds.time.attrs)\n",
    "\n",
    "        # add any extra coords you want to carry through\n",
    "        if 'InstrDepth' in ds:\n",
    "            ds_i = ds_i.assign_coords(InstrDepth=ds['InstrDepth'])\n",
    "        if 'clock_offset' in ds:\n",
    "            ds_i = ds_i.assign_coords(clock_offset=ds['clock_offset'])\n",
    "\n",
    "        datasets_interp.append(ds_i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6adbe82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import xarray as xr\n",
    "\n",
    "def merge_global_attrs(ds_list, *, order=\"last\"):\n",
    "    \"\"\"\n",
    "    Union of all global attrs across datasets.\n",
    "    If the same key appears multiple times, later datasets overwrite earlier ones (order='last').\n",
    "    Use order='first' to prefer the first occurrence instead.\n",
    "    \"\"\"\n",
    "    merged = {}\n",
    "    if order == \"last\":\n",
    "        it = ds_list\n",
    "    else:  # 'first'\n",
    "        it = reversed(ds_list)\n",
    "    for ds in it:\n",
    "        if hasattr(ds, \"attrs\") and ds.attrs:\n",
    "            merged.update(dict(ds.attrs))  # later updates overwrite\n",
    "    return merged\n",
    "\n",
    "def merge_var_attrs(varname, ds_list, *, order=\"last\"):\n",
    "    \"\"\"\n",
    "    Union of attrs for a given variable across datasets in ds_list.\n",
    "    Later datasets overwrite earlier ones (order='last').\n",
    "    \"\"\"\n",
    "    merged = {}\n",
    "    if order == \"last\":\n",
    "        it = ds_list\n",
    "    else:\n",
    "        it = reversed(ds_list)\n",
    "    for ds in it:\n",
    "        if varname in ds and getattr(ds[varname], \"attrs\", None):\n",
    "            merged.update(dict(ds[varname].attrs))\n",
    "    return merged\n",
    "\n",
    "def copy_coord_attrs(src_ds: xr.Dataset, dst_ds: xr.Dataset, coord_names=(\"time\",)):\n",
    "    \"\"\"Copy attrs for specific coords from src to dst if present.\"\"\"\n",
    "    for c in coord_names:\n",
    "        if c in src_ds.coords and getattr(src_ds[c], \"attrs\", None):\n",
    "            dst_ds[c].attrs = dict(src_ds[c].attrs)\n",
    "\n",
    "\n",
    "#combined_ds.attrs = common_attrs(datasets_clean)\n",
    "# Optionally carry time attrs\n",
    "#if 'time' in datasets_interp[0].coords and datasets_interp[0].time.attrs:\n",
    "#    combined_ds['time'].attrs = dict(datasets_interp[0].time.attrs)\n",
    "#combined_ds.attrs =\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b34a46",
   "metadata": {},
   "source": [
    "# Combine interpolated datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428070e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of variables to keep\n",
    "vars_to_keep = ['temperature', 'salinity', 'conductivity', 'pressure', 'u_velocity','v_velocity']#,'serial_number','InstrDepth']\n",
    "\n",
    "# Remove unwanted variables from each dataset\n",
    "datasets_clean = []\n",
    "for ds in datasets_interp:\n",
    "    ds_sel = ds.drop_vars(['density', 'potential_temperature', 'julian_days_offset','timeS'], errors='ignore')\n",
    "    datasets_clean.append(ds_sel)\n",
    "\n",
    "# Find union of all time coordinates (should be the same for all, but let's check)\n",
    "time_coord = datasets_interp[0]['time']\n",
    "\n",
    "# Prepare data arrays for each variable\n",
    "combined_data = {}\n",
    "N_LEVELS = len(datasets_clean)\n",
    "\n",
    "def first_ds_with(var):\n",
    "    for d in datasets_clean:\n",
    "        if var in d:\n",
    "            return d[var]\n",
    "    return None\n",
    "def stacked_or_nan(var):\n",
    "    arrs = []\n",
    "    for ds in datasets_clean:\n",
    "        if var in ds:\n",
    "            arrs.append(ds[var].values)\n",
    "        else:\n",
    "            arrs.append(np.full(time_coord.shape, np.nan, dtype=float))\n",
    "    return np.stack(arrs, axis=-1)  # (time, N_LEVELS)\n",
    "\n",
    "for var in vars_to_keep:\n",
    "    stacked = stacked_or_nan(var)\n",
    "    var_attrs = merge_var_attrs(var, datasets_clean, order=\"last\")\n",
    "    combined_data[var] = xr.DataArray(\n",
    "        stacked,\n",
    "        dims=('time','N_LEVELS'),\n",
    "        coords={'time': time_coord, 'N_LEVELS': np.arange(N_LEVELS)},\n",
    "        attrs=var_attrs\n",
    "    )\n",
    "\n",
    "\n",
    "# Per-level metadata coords\n",
    "depths, clock_offsets, serial, instrtype = [], [], [], []\n",
    "for ds in datasets_clean:\n",
    "    depths.append(float(ds['InstrDepth'].item()) if 'InstrDepth' in ds else np.nan)\n",
    "    serial.append(ds['serial_number'].item() if 'serial_number' in ds else np.nan)\n",
    "    instrtype.append(ds['instrument'].item() if 'instrument' in ds else 'unknown')\n",
    "    if 'clock_offset' in ds:\n",
    "        co = ds['clock_offset'].item()\n",
    "    elif 'seconds_offset' in ds:\n",
    "        co = ds['seconds_offset'].item()\n",
    "    else:\n",
    "        co = 0\n",
    "    clock_offsets.append(int(np.rint(co)) if np.isfinite(co) else 0)\n",
    "\n",
    "combined_ds = xr.Dataset(\n",
    "    data_vars=combined_data,\n",
    "    coords={\n",
    "        'time': time_coord,\n",
    "        'N_LEVELS': np.arange(N_LEVELS),\n",
    "        'clock_offset': ('N_LEVELS', np.asarray(clock_offsets)),\n",
    "        'serial_number': ('N_LEVELS', np.asarray(serial)),\n",
    "        'nominal_depth': ('N_LEVELS', np.asarray(depths)),\n",
    "        'instrument': ('N_LEVELS', np.asarray(instrtype)),\n",
    "    }\n",
    ")\n",
    "\n",
    "# Apply merged GLOBAL attrs (union across all inputs, last dataset wins on conflicts)\n",
    "combined_ds.attrs = merge_global_attrs(datasets_clean, order=\"last\")\n",
    "\n",
    "# Optionally also copy coordinate attrs (e.g., 'time' units, calendar)\n",
    "copy_coord_attrs(datasets_interp[0], combined_ds, coord_names=(\"time\",))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6cdd3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "\n",
    "def encode_instrument_as_flags(ds: xr.Dataset,\n",
    "                               var_name: str = \"instrument\",\n",
    "                               out_name: str = \"instrument_id\") -> xr.Dataset:\n",
    "    if var_name not in ds:\n",
    "        return ds\n",
    "\n",
    "    names = [str(v) for v in np.asarray(ds[var_name].values)]\n",
    "    uniq = []\n",
    "    for n in names:\n",
    "        if n not in uniq:\n",
    "            uniq.append(n)\n",
    "    codes = {name: i+1 for i, name in enumerate(uniq)}  # start at 1\n",
    "    id_arr = np.array([codes[n] for n in names], dtype=np.int16)\n",
    "\n",
    "    ds = ds.copy()\n",
    "    ds[out_name] = ((\"N_LEVELS\",), id_arr)\n",
    "    # CF style metadata\n",
    "    ds[out_name].attrs.update({\n",
    "        \"standard_name\": \"instrument_id\",\n",
    "        \"long_name\": \"Instrument identifier (encoded)\",\n",
    "        \"flag_values\": np.array(list(range(1, len(uniq)+1)), dtype=np.int16),\n",
    "        \"flag_meanings\": \" \".join(s.replace(\" \", \"_\") for s in uniq),\n",
    "        \"comment\": f\"Mapping: {codes}\"\n",
    "    })\n",
    "\n",
    "    # Optional: keep a readable list at global attrs\n",
    "    ds.attrs[\"instrument_names\"] = \", \".join(uniq)\n",
    "\n",
    "    # Drop the string variable so writers don’t touch it\n",
    "    ds = ds.drop_vars(var_name)\n",
    "\n",
    "    return ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94a4baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = name1 + '_mooring'  + file_subscript + '.nc'\n",
    "usefile = proc_dir + '/' + fname\n",
    "ds_to_save = encode_instrument_as_flags(combined_ds)\n",
    "\n",
    "print(usefile)\n",
    "if 1:\n",
    "    writer = NetCdfWriter(ds_to_save)\n",
    "    writer.write(\n",
    "        usefile,\n",
    "        optimize=True,\n",
    "        drop_derived=False,  # drops vars with attrs[\"derived\"] == True (e.g., z)\n",
    "        uint8_vars=[\n",
    "            \"correlation_magnitude\", \"echo_intensity\", \"status\", \"percent_good\",\n",
    "            \"bt_correlation\", \"bt_amplitude\", \"bt_percent_good\",\n",
    "        ],\n",
    "        float32_vars=[  # optional explicit list; float32=True already covers floats generically\n",
    "            \"eastward_velocity\", \"northward_velocity\", \"upward_velocity\",\n",
    "            \"temperature\", \"salinity\", \"pressure\", \"pressure_std\", \"depth\", \"bt_velocity\",\n",
    "        ],\n",
    "        chunk_time=3600,  # 1-hour chunks if you have ~1 Hz ensembles; adjust as needed\n",
    "        complevel=5,\n",
    "        quantize=3,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5430af",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44480db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.pcolormesh(combined_ds.time,combined_ds.N_LEVELS,combined_ds.temperature.transpose(), cmap='RdYlBu_r')\n",
    "vmin = np.nanpercentile(combined_ds.temperature.values, 5)\n",
    "vmax = np.nanpercentile(combined_ds.temperature.values, 95)\n",
    "plt.clim(vmin, vmax)\n",
    "plt.colorbar(label='Temperature')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51ed88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now average the temperature in time using a median average\n",
    "plt.plot(combined_ds.time,combined_ds.temperature[:,1])\n",
    "plt.plot(combined_ds.time,combined_ds.temperature[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89eba7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c80f38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.11.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
