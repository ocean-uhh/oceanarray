{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71edb016",
   "metadata": {},
   "source": [
    "## Demo: Stage2 processing for mooring data\n",
    "\n",
    "- Apply clock offsets\n",
    "- Trim to deployment period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff1f4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import yaml\n",
    "import scipy.io\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from ctd_tools.writers import NetCdfWriter\n",
    "from oceanarray import writers\n",
    "\n",
    "moorlist = ['ds2_X_2012','ds2_X_2017','ds2_X_2018',\n",
    "            'ds8_1_2012','ds9_1_2012','ds10_1_2012', 'ds11_1_2012','ds12_1_2012',\n",
    "            'ds13_1_2012','ds14_1_2012','ds15_1_2012','ds16_1_2012','ds17_1_2012',\n",
    "            'ds19_1_2012','ds18_1_2012','ds28_1_2017',\n",
    "            'dsA_1_2018','dsB_1_2018','dsC_1_2018', 'dsD_1_2018','dsE_1_2018','dsF_1_2018',\n",
    "            'dsM1_1_2017','dsM2_1_2017','dsM3_1_2017','dsM4_1_2017','dsM5_1_2017']\n",
    "moorlist = ['dsE_1_2018']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518ff131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the base directory.  raw is a subdirectory from here moor/raw/ and proc is moor/proc\n",
    "basedir = '/Users/eddifying/Dropbox/data/ifmro_mixsed/ds_data_eleanor/'\n",
    "output_path = basedir + 'moor/proc/'\n",
    "\n",
    "\n",
    "\n",
    "def read_yaml_time(data, key):\n",
    "    \"\"\"Return datetime64[ns] from YAML dict or NaT if missing/invalid.\"\"\"\n",
    "    val = data.get(key, None)\n",
    "    print(val)\n",
    "    if val is None or (isinstance(val, str) and not val.strip()):\n",
    "        return np.datetime64(\"NaT\", \"ns\")\n",
    "    try:\n",
    "        return pd.to_datetime(val).to_datetime64()\n",
    "    except Exception:\n",
    "        return np.datetime64(\"NaT\", \"ns\")\n",
    "\n",
    "# Cycle through the yaml and load instrument data into a list of xarray datasets\n",
    "# Enrich the netCDF with information from the yaml file\n",
    "# Find the mooring's processed directory & read the yaml specification\n",
    "name1 = moorlist[0]\n",
    "proc_dir = output_path + name1\n",
    "moor_yaml = proc_dir + '/' + name1 + '.mooring.yaml'\n",
    "with open(moor_yaml, 'r') as f:\n",
    "    moor_yaml_data = yaml.safe_load(f)\n",
    "\n",
    "# For each instrument, load the raw netCDF files and add some metadata from the yaml\n",
    "datasets = []\n",
    "deploy_time  = read_yaml_time(moor_yaml_data, \"deployment_time\")\n",
    "recover_time = read_yaml_time(moor_yaml_data, \"recovery_time\")\n",
    "print(f\"deploy time is {deploy_time}\")\n",
    "\n",
    "for i in moor_yaml_data['instruments']:\n",
    "    fname = name1 + '_' + str(i['serial']) + '_raw.nc'\n",
    "    rawfile = proc_dir + '/' + i['instrument'] + '/' + fname\n",
    "\n",
    "    if os.path.exists(rawfile):\n",
    "        ds1 = xr.open_dataset(rawfile)\n",
    "\n",
    "        if 'InstrDepth' not in ds1.variables and 'depth' in i:\n",
    "            ds1['InstrDepth'] = i['depth']\n",
    "        if 'instrument' not in ds1.variables and 'instrument' in i:\n",
    "            ds1['instrument'] = i['instrument']\n",
    "        if 'serial_number' not in ds1.variables and 'serial' in i:\n",
    "            ds1['serial_number'] = i['serial']\n",
    "        if 'timeS' in ds1.variables:\n",
    "            ds1 = ds1.drop_vars('timeS')\n",
    "\n",
    "        # Apply the clock offset\n",
    "        ds1['clock_offset'] = i.get('clock_offset', 0)\n",
    "        ds1['clock_offset'].attrs['units'] = 's'\n",
    "        clock_offset = ds1['clock_offset'].values\n",
    "        ds1['time'] = ds1['time'] + np.timedelta64(int(ds1['clock_offset'].values), 's')\n",
    "\n",
    "        if np.isfinite(deploy_time):\n",
    "            ds1 = ds1.sel(time=slice(deploy_time, None))\n",
    "        if np.isfinite(recover_time):\n",
    "            ds1 = ds1.sel(time=slice(None, recover_time))\n",
    "\n",
    "        start_time = ds1['time'].values.min()\n",
    "        end_time = ds1['time'].values.max()\n",
    "        print(f\"Deploy time is {deploy_time}. Data starts at {start_time} and ends at {end_time}\")\n",
    "        #---------------------------------------------\n",
    "        # Store the data in a list of datasets\n",
    "        fname2 = fname.replace('_raw','_use')\n",
    "        fileout = proc_dir + '/' + i['instrument'] + '/' + fname2\n",
    "        print(f\"Saving to {proc_dir} + {fname2}\")\n",
    "\n",
    "        if os.path.exists(fileout):\n",
    "            os.remove(fileout)\n",
    "\n",
    "        writer = NetCdfWriter(ds1)\n",
    "        writer.write(\n",
    "            fileout,\n",
    "            optimize=True,\n",
    "            drop_derived=False,  # drops vars with attrs[\"derived\"] == True (e.g., z)\n",
    "            uint8_vars=[\n",
    "                \"correlation_magnitude\", \"echo_intensity\", \"status\", \"percent_good\",\n",
    "                \"bt_correlation\", \"bt_amplitude\", \"bt_percent_good\",\n",
    "            ],\n",
    "            float32_vars=[  # optional explicit list; float32=True already covers floats generically\n",
    "                \"eastward_velocity\", \"northward_velocity\", \"upward_velocity\",\n",
    "                \"temperature\", \"salinity\", \"pressure\", \"pressure_std\", \"depth\", \"bt_velocity\",\n",
    "            ],\n",
    "            chunk_time=3600,  # 1-hour chunks if you have ~1 Hz ensembles; adjust as needed\n",
    "            complevel=5,\n",
    "            quantize=3,\n",
    "        )\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982fccbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "val = '2018-08-12T19:53:00'\n",
    "f = pd.to_datetime(val).to_datetime64()\n",
    "print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d042d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    fname = 'dsC_1_2018_7516_use.nc'\n",
    "    instr = 'microcat'\n",
    "    filein = proc_dir + '/' + instr + '/' + fname\n",
    "    ds2 = xr.open_dataset(filein)\n",
    "\n",
    "    fname = 'dsC_1_2018_7516_raw.nc'\n",
    "    instr = 'microcat'\n",
    "    filein = proc_dir + '/' + instr + '/' + fname\n",
    "    ds1 = xr.open_dataset(filein)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.11.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
