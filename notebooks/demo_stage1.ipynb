{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71edb016",
   "metadata": {},
   "source": "## Demo: Stage1 processing for mooring data\n\n**Stage 1 Overview**: This is the first processing stage that converts raw instrument files into standardized CF-compliant NetCDF format. It handles multiple instrument types using the `ctd_tools` library for reading and the oceanarray framework for metadata management.\n\n### What Stage1 Does:\n- **File Conversion**: Reads raw instrument files (SeaBird, RBR, Nortek, etc.) and converts to NetCDF\n- **Standardization**: Applies CF conventions for variable names, units, and metadata\n- **Format Preservation**: Preserves original data values without modification or filtering\n- **Metadata Enrichment**: Adds deployment information from YAML configuration files\n- **Organization**: Outputs files organized by instrument type in the processed directory\n\n### Input Files:\n- Raw instrument data files (various formats: `.cnv`, `.rsk`, `.dat`, `.mat`)\n- YAML configuration files specifying mooring and instrument metadata\n\n### Output Files:\n- Standardized NetCDF files: `{mooring}_{serial}_raw.nc`\n- Processing log files for debugging and quality assurance\n\n### Processing Flow:\n1. Raw files ‚Üí ctd_tools readers ‚Üí xarray.Dataset\n2. Apply CF conventions and metadata from YAML\n3. Preserve all original data values unchanged\n4. Save as NetCDF with standardized naming\n\n**Note**: Stage1 focuses purely on format conversion with no data modification. All processing (filtering, clock corrections, quality control, trimming) happens in Stage2 and subsequent stages.\n\nThis notebook demonstrates the usage of the refactored `oceanarray.stage1` module."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff1f4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "from pathlib import Path\n",
    "\n",
    "# Import the refactored stage1 processing module\n",
    "from oceanarray.stage1 import MooringProcessor, process_multiple_moorings, stage1_mooring\n",
    "\n",
    "# For testing individual readers (optional)\n",
    "from ctd_tools.readers import NortekAsciiReader, AdcpMatlabReader\n",
    "from ctd_tools.plotters import TimeSeriesPlotter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "config_section",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Set up the base directory and mooring lists for processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37139297",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base directory containing the mooring data\n",
    "basedir = '/Users/eddifying/Dropbox/data/ifmro_mixsed/ds_data_eleanor/'\n",
    "\n",
    "# Define mooring lists\n",
    "all_moorings = ['ds2_X_2012', 'ds2_X_2017', 'ds2_X_2018',\n",
    "                'ds8_1_2012', 'ds9_1_2012', 'ds10_1_2012', 'ds11_1_2012', 'ds12_1_2012',\n",
    "                'ds13_1_2012', 'ds14_1_2012', 'ds15_1_2012', 'ds16_1_2012', 'ds17_1_2012',\n",
    "                'ds19_1_2012', 'ds18_1_2012', 'ds28_1_2017',\n",
    "                'dsA_1_2018', 'dsB_1_2018', 'dsC_1_2018', 'dsD_1_2018', 'dsE_1_2018', 'dsF_1_2018',\n",
    "                'dsM1_1_2017', 'dsM2_1_2017', 'dsM3_1_2017', 'dsM4_1_2017', 'dsM5_1_2017']\n",
    "\n",
    "# Subset for testing\n",
    "test_moorings_2018 = ['dsA_1_2018', 'dsB_1_2018', 'dsC_1_2018', 'dsD_1_2018', 'dsE_1_2018', 'dsF_1_2018']\n",
    "single_test = ['dsB_1_2018']\n",
    "\n",
    "# Choose which set to process\n",
    "moorlist = single_test\n",
    "\n",
    "print(f\"Base directory: {basedir}\")\n",
    "print(f\"Processing {len(moorlist)} moorings: {moorlist}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "processing_section",
   "metadata": {},
   "source": [
    "## Processing Moorings\n",
    "\n",
    "### Option 1: Using the new class-based approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e4a3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the processor\n",
    "processor = MooringProcessor(basedir)\n",
    "\n",
    "# Process each mooring individually with detailed output\n",
    "results = {}\n",
    "for mooring_name in moorlist:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Processing mooring: {mooring_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    success = processor.process_mooring(mooring_name)\n",
    "    results[mooring_name] = success\n",
    "\n",
    "    status = \"‚úÖ SUCCESS\" if success else \"‚ùå FAILED\"\n",
    "    print(f\"\\nResult for {mooring_name}: {status}\")\n",
    "\n",
    "# Print final summary\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"FINAL PROCESSING SUMMARY\")\n",
    "print(f\"{'='*60}\")\n",
    "successful = sum(results.values())\n",
    "total = len(results)\n",
    "print(f\"Successfully processed: {successful}/{total} moorings\")\n",
    "\n",
    "for mooring, success in results.items():\n",
    "    status = \"‚úÖ\" if success else \"‚ùå\"\n",
    "    print(f\"{status} {mooring}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "batch_processing",
   "metadata": {},
   "source": [
    "### Option 2: Using the convenience function for batch processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "batch_cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative: Use the batch processing function\n",
    "# Uncomment to use this approach instead\n",
    "\n",
    "if 0<0:\n",
    "    results = process_multiple_moorings(moorlist, basedir)\n",
    "\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(\"BATCH PROCESSING RESULTS\")\n",
    "    print(f\"{'='*50}\")\n",
    "    for mooring, success in results.items():\n",
    "        status = \"SUCCESS\" if success else \"FAILED\"\n",
    "        print(f\"{mooring}: {status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "backwards_compat",
   "metadata": {},
   "source": [
    "### Option 3: Using the backwards-compatible function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compat_cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative: Use the original function signature for backwards compatibility\n",
    "# Uncomment to use this approach instead\n",
    "\n",
    "if 0:\n",
    "    for mooring_name in moorlist:\n",
    "        print(f\"Processing {mooring_name}...\")\n",
    "        success = stage1_mooring(mooring_name, basedir=basedir)\n",
    "        status = \"SUCCESS\" if success else \"FAILED\"\n",
    "        print(f\"{mooring_name}: {status}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "testing_section",
   "metadata": {},
   "source": [
    "## Testing Individual Readers\n",
    "\n",
    "Test reading specific instrument files directly to debug issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81775c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test reading a Nortek AquaDopp file directly\n",
    "try:\n",
    "    rawdir = Path(basedir) / 'moor/raw/msm76_2018/'\n",
    "    instrument = 'aquadopp'\n",
    "    data_dir = rawdir / instrument\n",
    "    fname = 'DSC18_477102.dat'\n",
    "    filename = data_dir / fname\n",
    "    header_file = data_dir / (fname[:-4] + '.hdr')\n",
    "\n",
    "    print(f\"Data file: {filename}\")\n",
    "    print(f\"Header file: {header_file}\")\n",
    "    print(f\"Files exist: data={filename.exists()}, header={header_file.exists()}\")\n",
    "\n",
    "    if filename.exists() and header_file.exists():\n",
    "        reader = NortekAsciiReader(str(filename), header_file_path=str(header_file))\n",
    "        dataset = reader.get_data()\n",
    "\n",
    "        print(f\"\\nDataset loaded successfully!\")\n",
    "        print(f\"Variables: {list(dataset.data_vars)}\")\n",
    "        print(f\"Time range: {dataset.time.min().values} to {dataset.time.max().values}\")\n",
    "\n",
    "        # Plot if east_velocity exists\n",
    "        if 'east_velocity' in dataset.data_vars:\n",
    "            plotter = TimeSeriesPlotter(dataset)\n",
    "            plotter.plot(parameter_name='east_velocity')\n",
    "            plt.title(f'East Velocity - {fname}')\n",
    "            plt.show()\n",
    "\n",
    "        # Display dataset info\n",
    "        display(dataset)\n",
    "    else:\n",
    "        print(\"Files not found - skipping test\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error testing Nortek reader: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09961d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test reading an ADCP MATLAB file directly\n",
    "try:\n",
    "    rawdir = Path(basedir) / 'moor/raw/msm76_2018/'\n",
    "    instrument = 'adcp'\n",
    "    data_dir = rawdir / instrument\n",
    "    fname = 'DS0218_RDI_000_24289.mat'\n",
    "    filename = data_dir / fname\n",
    "\n",
    "    print(f\"ADCP file: {filename}\")\n",
    "    print(f\"File exists: {filename.exists()}\")\n",
    "\n",
    "    if filename.exists():\n",
    "        reader = AdcpMatlabReader(str(filename))\n",
    "        dataset = reader.get_data()\n",
    "\n",
    "        print(f\"\\nADCP Dataset loaded successfully!\")\n",
    "        print(f\"Variables: {list(dataset.data_vars)}\")\n",
    "        print(f\"Time range: {dataset.time.min().values} to {dataset.time.max().values}\")\n",
    "\n",
    "        # Plot pressure if it exists\n",
    "        if 'pressure' in dataset.data_vars:\n",
    "            plt.figure(figsize=(12, 4))\n",
    "            plt.plot(dataset.time, dataset.pressure)\n",
    "            plt.title(f'Pressure - {fname}')\n",
    "            plt.xlabel('Time')\n",
    "            plt.ylabel('Pressure')\n",
    "            plt.grid(True)\n",
    "            plt.show()\n",
    "\n",
    "        # Display dataset info\n",
    "        display(dataset)\n",
    "    else:\n",
    "        print(\"File not found - skipping test\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error testing ADCP reader: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "analysis_section",
   "metadata": {},
   "source": [
    "## Analyzing Processed Results\n",
    "\n",
    "Load and visualize the processed NetCDF files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b34a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze processed files\n",
    "def analyze_processed_mooring(mooring_name, basedir, instrument_type='microcat'):\n",
    "    \"\"\"Analyze processed files for a specific mooring and instrument type.\"\"\"\n",
    "    proc_dir = Path(basedir) / 'moor/proc' / mooring_name / instrument_type\n",
    "    fig_dir = proc_dir\n",
    "\n",
    "    if not proc_dir.exists():\n",
    "        print(f\"Directory not found: {proc_dir}\")\n",
    "        return\n",
    "\n",
    "    print(f\"Analyzing processed files in: {proc_dir}\")\n",
    "\n",
    "    # Find all NetCDF files\n",
    "    nc_files = list(proc_dir.glob('*raw.nc'))\n",
    "    if not nc_files:\n",
    "        print(\"No NetCDF files found\")\n",
    "        return\n",
    "\n",
    "    print(f\"Found {len(nc_files)} NetCDF files:\")\n",
    "\n",
    "    for file_path in nc_files:\n",
    "        try:\n",
    "            print(f\"\\nüìÑ Loading: {file_path.name}\")\n",
    "\n",
    "            with xr.open_dataset(file_path) as dataset:\n",
    "                print(f\"   Variables: {list(dataset.data_vars)}\")\n",
    "                print(f\"   Attributes: {list(dataset.attrs.keys())}\")\n",
    "\n",
    "                # Extract key metadata\n",
    "                mooring_name_attr = dataset.attrs.get('mooring_name', 'Unknown')\n",
    "                instrument_name = dataset.get('instrument', 'Unknown').values if 'instrument' in dataset else 'Unknown'\n",
    "                serial_number = dataset.get('serial_number', 0).values if 'serial_number' in dataset else 0\n",
    "\n",
    "                print(f\"   Mooring: {mooring_name_attr}\")\n",
    "                print(f\"   Instrument: {instrument_name}\")\n",
    "                print(f\"   Serial: {serial_number}\")\n",
    "\n",
    "                # Plot temperature if available\n",
    "                if 'temperature' in dataset.data_vars:\n",
    "                    fig, ax = plt.subplots(figsize=(12, 4))\n",
    "                    ax.plot(dataset['time'], dataset['temperature'], 'b-', linewidth=0.5)\n",
    "                    ax.set_ylabel('Temperature (¬∞C)')\n",
    "                    ax.set_title(f'{mooring_name_attr}: {instrument_name} {serial_number} - Temperature')\n",
    "                    ax.set_xlabel('Time')\n",
    "                    ax.grid(True, alpha=0.3)\n",
    "\n",
    "                    # Add stats\n",
    "                    temp_mean = dataset['temperature'].mean().values\n",
    "                    temp_std = dataset['temperature'].std().values\n",
    "                    ax.text(0.02, 0.98, f'Mean: {temp_mean:.2f}¬∞C\\nStd: {temp_std:.2f}¬∞C',\n",
    "                           transform=ax.transAxes, verticalalignment='top',\n",
    "                           bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "\n",
    "                    plt.tight_layout()\n",
    "                    plt.show()\n",
    "\n",
    "                    # Save plot\n",
    "                    plot_name = f\"{mooring_name_attr}_{instrument_name}_{serial_number}_temperature_raw.png\"\n",
    "                    fig.savefig(fig_dir / plot_name, dpi=150, bbox_inches='tight')\n",
    "                    print(f\"   üìä Plot saved: {plot_name}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Error loading {file_path.name}: {e}\")\n",
    "\n",
    "# Example usage\n",
    "if results and any(results.values()):\n",
    "    # Analyze the first successfully processed mooring\n",
    "    successful_mooring = next(mooring for mooring, success in results.items() if success)\n",
    "    print(f\"\\nAnalyzing successful mooring: {successful_mooring}\")\n",
    "    analyze_processed_mooring(successful_mooring, basedir, 'microcat')\n",
    "else:\n",
    "    print(\"No successfully processed moorings to analyze\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201ab628",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.11.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}