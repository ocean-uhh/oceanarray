{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71edb016",
   "metadata": {},
   "source": [
    "## Demo: Stage1 processing for mooring data\n",
    "\n",
    "Read the original raw files and convert to netCDF.  None to minimal additional processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff1f4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import yaml\n",
    "import scipy.io\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "from datetime import datetime\n",
    "\n",
    "from ctd_tools.readers import SbeCnvReader, RbrRskAutoReader, RbrMatlabReader, NortekAsciiReader, SbeAsciiReader, RbrAsciiReader, AdcpMatlabReader\n",
    "from ctd_tools.writers import NetCdfWriter\n",
    "from ctd_tools.plotters import TimeSeriesPlotter\n",
    "\n",
    "moorlist = ['ds2_X_2012','ds2_X_2017','ds2_X_2018',\n",
    "            'ds8_1_2012','ds9_1_2012','ds10_1_2012', 'ds11_1_2012','ds12_1_2012',\n",
    "            'ds13_1_2012','ds14_1_2012','ds15_1_2012','ds16_1_2012','ds17_1_2012',\n",
    "            'ds19_1_2012','ds18_1_2012','ds28_1_2017',\n",
    "            'dsA_1_2018','dsB_1_2018','dsC_1_2018', 'dsD_1_2018','dsE_1_2018','dsF_1_2018',\n",
    "            'dsM1_1_2017','dsM2_1_2017','dsM3_1_2017','dsM4_1_2017','dsM5_1_2017']\n",
    "moorlist = ['dsA_1_2018','dsB_1_2018','dsC_1_2018', 'dsD_1_2018','dsE_1_2018','dsF_1_2018']\n",
    "moorlist = ['dsE_1_2018']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37139297",
   "metadata": {},
   "outputs": [],
   "source": [
    "basedir = '/Users/eddifying/Dropbox/data/ifmro_mixsed/ds_data_eleanor/'\n",
    "def stage1_mooring(mooring_name, basedir=None, output_path=None):\n",
    "    '''\n",
    "    Process a single mooring's data.\n",
    "\n",
    "    The default output_path will be a subdirectory from the basedir as:\n",
    "    basedir + moor/proc/mooring_name/instrument/\n",
    "\n",
    "    To override this, input a full path for the output_path, but note that\n",
    "    from the output path, the files will arrive in:\n",
    "    output_path + mooring_name/instrument/\n",
    "\n",
    "    Performs basic loading from the original data file (as specified in\n",
    "    the YAML \"filename\").  It will look for this file in:\n",
    "    basedir + moor/raw/instrument/\n",
    "\n",
    "    Outfiles will be named: mooringname_serial_raw.nc\n",
    "    '''\n",
    "    if output_path is None:\n",
    "        output_path = basedir + 'moor/proc/' + mooring_name\n",
    "    moor_yaml = output_path + '/' + mooring_name + '.mooring.yaml'\n",
    "\n",
    "    # Set up log file\n",
    "    log_time = datetime.now().strftime('%Y%m%dT%H')\n",
    "    log_file = os.path.join(output_path, f\"{mooring_name}_{log_time}_stage1.mooring.log\")\n",
    "    def log_print(*args, **kwargs):\n",
    "        print(*args, **kwargs)\n",
    "        with open(log_file, 'a') as lf:\n",
    "            print(*args, **kwargs, file=lf)\n",
    "\n",
    "    log_print(mooring_name)\n",
    "    with open(moor_yaml, 'r') as f:\n",
    "        yaml_data = yaml.safe_load(f)\n",
    "\n",
    "    indir = basedir + yaml_data['directory']\n",
    "    dir_file = yaml_data['directory']\n",
    "    for i in yaml_data['instruments']:\n",
    "        readflag = False\n",
    "        if 'filename' in i:\n",
    "            infile_for_log = dir_file + i['instrument'] +'/'+i['filename']\n",
    "            infile = indir + i['instrument'] + '/' + i['filename']\n",
    "            out_inst_dir = output_path + '/' + i['instrument'] + '/'\n",
    "            if not os.path.exists(out_inst_dir):\n",
    "                os.makedirs(out_inst_dir)\n",
    "                log_print(f\"Created directory: {out_inst_dir}\")\n",
    "\n",
    "            # Cycle through reader types. Note that the SBE reader adds extra variables. Delete these.\n",
    "            try:\n",
    "                remove_vars = []\n",
    "                remove_coords = []\n",
    "                if i['file_type'] == 'sbe-cnv':\n",
    "                    reader = SbeCnvReader(infile)\n",
    "                    readflag = True\n",
    "                    remove_vars = ['potential_temperature', 'julian_days_offset', 'density']\n",
    "                    remove_coords = ['depth', 'latitude', 'longitude']\n",
    "                elif i['file_type'] == 'nortek-aqd':\n",
    "                    header_path = indir + i['instrument'] + '/' + i['header']\n",
    "                    reader = NortekAsciiReader(infile, header_file_path=header_path)\n",
    "                    readflag = True\n",
    "                elif i['file_type'] == 'sbe-asc':\n",
    "                    reader = SbeAsciiReader(infile)\n",
    "                    readflag = True\n",
    "                    remove_vars = ['potential_temperature', 'julian_days_offset', 'density']\n",
    "                    remove_coords = ['depth', 'latitude', 'longitude']\n",
    "                elif i['file_type'] == 'rbr-rsk':\n",
    "                    reader = RbrRskAutoReader(infile)\n",
    "                    readflag = True\n",
    "                elif i['file_type'] == 'rbr-matlab':\n",
    "                    reader = RbrMatlabReader(infile)\n",
    "                    readflag = True\n",
    "                elif i['file_type'] == 'rbr-dat':\n",
    "                    reader = RbrAsciiReader(infile)\n",
    "                    readflag = True\n",
    "                elif i['file_type'] == 'adcp-matlab':\n",
    "                    reader = AdcpMatlabReader(infile)\n",
    "                    readflag = True\n",
    "            except AttributeError as e:\n",
    "                log_print(f\"EXCEPT: Error reading file {infile_for_log}: {e}\")\n",
    "                continue\n",
    "\n",
    "            def find_known_tag(fname, tags=(\"_000\", \"_001\", \"_002\")):\n",
    "                fname = str(fname)\n",
    "                tag = ''\n",
    "                for t in tags:\n",
    "                    if t in fname:\n",
    "                        tag = t\n",
    "                return tag\n",
    "\n",
    "            if readflag:\n",
    "                tag1 = find_known_tag(i['filename'])\n",
    "                if i['file_type'] == 'adcp-matlab':\n",
    "                    tag1 = tag1\n",
    "                else:\n",
    "                    tag1 = ''\n",
    "                output_file = out_inst_dir + mooring_name + \"_\" + str(i['serial']) + tag1 + '_raw.nc'\n",
    "                outfile_for_log = output_file.replace(basedir, '')\n",
    "                if not os.path.exists(output_file):\n",
    "                    log_print(f\"-->   Processing {i['instrument']}: {infile_for_log}\")\n",
    "                    log_print(f\"Creating output file: {outfile_for_log}\")\n",
    "\n",
    "                    # Read the data into an xarray dataset\n",
    "                    dataset = reader.get_data()\n",
    "                    if remove_vars:\n",
    "                        for var in remove_vars:\n",
    "                            if var in dataset.variables:\n",
    "                                log_print(f\"Removing variable: {var}\")\n",
    "                                dataset = dataset.drop_vars(var)\n",
    "                    if remove_coords:\n",
    "                        for coord in remove_coords:\n",
    "                            if coord in dataset.coords:\n",
    "                                log_print(f\"Removing coordinate: {coord}\")\n",
    "                                dataset = dataset.drop_vars(coord)\n",
    "\n",
    "                    # Get parameters from yaml\n",
    "                    # Global attributes\n",
    "                    dataset.attrs['mooring_name'] = yaml_data['name']\n",
    "                    dataset.attrs['waterdepth'] = yaml_data['waterdepth']\n",
    "                    dataset.attrs['longitude'] = yaml_data.get('longitude', 0.0)\n",
    "                    dataset.attrs['latitude'] = yaml_data.get('latitude', 0.0)\n",
    "                    dataset.attrs['deployment_latitude'] = yaml_data.get('deployment_latitude', '00 00.000 N')\n",
    "                    dataset.attrs['deployment_longitude'] = yaml_data.get('deployment_longitude', '000 00.000 W')\n",
    "                    dataset.attrs['deployment_time'] = yaml_data.get('deployment_time','YYYY-mm-ddTHH:MM:ss')\n",
    "                    dataset.attrs['seabed_latitude'] = yaml_data.get('seabed_latitude', '00 00.000 N')\n",
    "                    dataset.attrs['seabed_longitude'] = yaml_data.get('seabed_longitude', '000 00.000 W')\n",
    "                    dataset.attrs['recovery_time'] = yaml_data.get('recovery_time', 'YYYY-mm-ddTHH:MM:ss')\n",
    "\n",
    "                    dataset['serial_number'] = i.get('serial',0)\n",
    "                    dataset['InstrDepth'] = i.get('depth',0)\n",
    "                    dataset['instrument'] = i.get('instrument','Unknown')\n",
    "                    dataset['clock_offset'] = i.get('clock_offset', 0)\n",
    "                    dataset['clock_offset'].attrs['units'] = 's'\n",
    "                    if 0:\n",
    "                        # Applying clock offsets shouldn't really happen until *_use.nc is created\n",
    "                        dataset['time'] = dataset['time'] + np.timedelta64(int(dataset['clock_offset'].values), 's')\n",
    "\n",
    "                    dataset['start_time'] = i.get('start_time', dataset.attrs['deployment_time'])\n",
    "                    dataset['end_time'] = i.get('end_time', dataset.attrs['recovery_time'])\n",
    "\n",
    "                    writer = NetCdfWriter(dataset)\n",
    "                    writer.write(\n",
    "                        output_file,\n",
    "                        optimize=True,\n",
    "                        drop_derived=False,  # drops vars with attrs[\"derived\"] == True (e.g., z)\n",
    "                        uint8_vars=[\n",
    "                            \"correlation_magnitude\", \"echo_intensity\", \"status\", \"percent_good\",\n",
    "                            \"bt_correlation\", \"bt_amplitude\", \"bt_percent_good\",\n",
    "                        ],\n",
    "                        float32_vars=[  # optional explicit list; float32=True already covers floats generically\n",
    "                            \"eastward_velocity\", \"northward_velocity\", \"upward_velocity\",\n",
    "                            \"temperature\", \"salinity\", \"pressure\", \"pressure_std\", \"depth\", \"bt_velocity\",\n",
    "                        ],\n",
    "                        chunk_time=3600,  # 1-hour chunks if you have ~1 Hz ensembles; adjust as needed\n",
    "                        complevel=5,\n",
    "                        quantize=3,\n",
    "                    )\n",
    "                else:\n",
    "                    # Delete output file\n",
    "                    #os.remove(output_file)\n",
    "                    log_print(f\"OUTFILE EXISTS: Skipping   {outfile_for_log}.\")\n",
    "            else:\n",
    "                log_print(f\"READER: No valid readers:  {infile_for_log}.\")\n",
    "        else:\n",
    "            fname_for_log2 = dir_file + i['instrument'] +'/'+str(i['serial'])\n",
    "            log_print(f\"FILENAME MISSING: Skipping {i['instrument']}:{fname_for_log2}.  YAML is missing 'filename'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e4a3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "basedir = '/Users/eddifying/Dropbox/data/ifmro_mixsed/ds_data_eleanor/'\n",
    "output_path = basedir + 'moor/proc/'\n",
    "\n",
    "\n",
    "for idx, name1 in enumerate(moorlist):\n",
    "    stage1_mooring(name1, basedir=basedir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81775c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FileNotFoundError: [Errno 2] No such file or directory: '/Users/eddifying/Dropbox/data/ifmro_mixsed/ds_data_eleanor/moor/raw/msm76_2018/adcp/DSF18_RDI_000_14971_001.mat'\n",
    "\n",
    "## Try a plotter on a raw file\n",
    "rawdir = basedir + 'moor/raw/msm76_2018/'\n",
    "instrument = 'aquadopp'\n",
    "data_dir = rawdir + instrument\n",
    "fname = 'DSC18_477102.dat'\n",
    "filename = data_dir + '/' + fname\n",
    "header_file = data_dir + '/' + fname[0:-4] + '.hdr'\n",
    "print(header_file)\n",
    "print(filename)\n",
    "\n",
    "#output_path = basedir + 'moor/proc/dsC_1_2018/aquadopp/' + fname[0:-4] + '.nc'\n",
    "reader = NortekAsciiReader(filename,header_file_path=header_file)\n",
    "dataset = reader.get_data()\n",
    "plotter = TimeSeriesPlotter(dataset)\n",
    "plotter.plot(parameter_name='east_velocity')\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09961d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "basedir = '/Users/eddifying/Dropbox/data/ifmro_mixsed/ds_data_eleanor/'\n",
    "\n",
    "## Try a plotter on a raw file\n",
    "rawdir = basedir + 'moor/raw/msm76_2018/'\n",
    "instrument = 'adcp'\n",
    "data_dir = rawdir + instrument\n",
    "fname = 'DS0218_RDI_000_24289.mat'\n",
    "filename = data_dir + '/' + fname\n",
    "print(filename)\n",
    "\n",
    "#output_path = basedir + 'moor/proc/dsC_1_2018/aquadopp/' + fname[0:-4] + '.nc'\n",
    "reader = AdcpMatlabReader(filename)\n",
    "dataset = reader.get_data()\n",
    "#plotter = TimeSeriesPlotter(dataset)\n",
    "#plotter.plot(parameter_name='east_velocity')\n",
    "plt.plot(dataset.pressure)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05665df",
   "metadata": {},
   "outputs": [],
   "source": [
    "basedir = '/Users/eddifying/Dropbox/data/ifmro_mixsed/ds_data_eleanor/moor/'\n",
    "procdir = 'proc/dsC_1_2018/'\n",
    "\n",
    "instrument = 'aquadopp'\n",
    "fname = 'DSC18_905701.nc'\n",
    "fname2 = 'dsC_1_2018_9057_raw.nc'\n",
    "\n",
    "ds1 = xr.open_dataset(basedir+procdir+instrument+'/'+fname)\n",
    "ds2 = xr.open_dataset(basedir+procdir+instrument+'/'+fname2)\n",
    "\n",
    "\n",
    "plt.plot(ds1.time, ds1.east_velocity, label='east_velocity')\n",
    "plt.plot(ds2.time, ds2.east_velocity, ':r',label='east_velocity2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c81c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b34a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "filenames=['dsM4_1_2017_7513_raw.nc','dsM4_1_2017_7517_raw.nc']\n",
    "# Now try loading all netcdf files in the proc directory\n",
    "proc_dir = basedir + 'moor/proc/dsM4_1_2017/microcat/'\n",
    "for file in os.listdir(proc_dir):\n",
    "    if file.endswith('.nc'):\n",
    "        file_path = os.path.join(proc_dir, file)\n",
    "        print(f\"Loading netCDF file: {file_path}\")\n",
    "        dataset = xr.open_dataset(file_path)\n",
    "        # You can add more processing or analysis here\n",
    "        print(list(dataset.data_vars))  # Print variable names in the dataset\n",
    "        print(list(dataset.attrs))\n",
    "\n",
    "        if 'temperature' in dataset.data_vars:\n",
    "            fig, ax = plt.subplots(figsize=(8, 2))\n",
    "            ax.plot(dataset['time'], dataset['temperature'], label='Temperature')\n",
    "            ax.set_ylabel('Temperature (°C)')\n",
    "            ax.set_title(dataset.attrs['mooring_name'] + ': ' + dataset['instrument'].values + ' ' + str(dataset['serial_number'].values))\n",
    "            ax.set_xlabel('Time')\n",
    "            ax.legend()\n",
    "            ax.grid()\n",
    "\n",
    "# Save the plot to a png\n",
    "            fig.savefig(f\"{dataset.attrs['mooring_name']}_{dataset['instrument'].values}_T.png\")\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3674accd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.11.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
